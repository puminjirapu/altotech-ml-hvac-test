{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load columns_info.json\n",
    "with open('../data/columns_info.json') as f:\n",
    "    columns_info = json.load(f)\n",
    "\n",
    "# Load chiller_plant_dataset.parquet\n",
    "chiller_data = pd.read_parquet('../data/chiller_plant_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "import ast\n",
    "\n",
    "column_list = chiller_data.columns.to_list()\n",
    "columns_info_list = []\n",
    "for key in columns_info:\n",
    "    tup = ast.literal_eval(key)\n",
    "    columns_info_list.append(tup)\n",
    "columns_to_drop = [col for col in column_list if col not in columns_info_list]\n",
    "columns_to_drop.append(('chiller_6', 'power'))\n",
    "chiller_data = chiller_data.drop(columns=columns_to_drop)\n",
    "chiller_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean missing value\n",
    "chiller_data.ffill(inplace=True)\n",
    "chiller_data.bfill(inplace=True)\n",
    "chiller_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negative value\n",
    "# Check which columns have negative values\n",
    "negative_columns = chiller_data.columns[chiller_data.lt(0).any()]\n",
    "\n",
    "# Count negative values in each column\n",
    "negative_counts = chiller_data[negative_columns].lt(0).sum().sum()\n",
    "\n",
    "print(\"negative values:\")\n",
    "print(negative_counts)\n",
    "# Replace negative values with NaN\n",
    "chiller_data[negative_columns] = chiller_data[negative_columns].mask(chiller_data[negative_columns] < 0)\n",
    "# Forward fill NaN values\n",
    "chiller_data.ffill(inplace=True)\n",
    "chiller_data.bfill(inplace=True)\n",
    "\n",
    "# Verify if negative values are replaced\n",
    "negative_counts_after_fill = chiller_data[negative_columns].lt(0).sum().sum()\n",
    "\n",
    "print(\"negative values after forward filling:\")\n",
    "print(negative_counts_after_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Multidexing to normal columns name\n",
    "chiller_data.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in chiller_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data['plant_target_chw_setpoint'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data.to_parquet('chiller_data_pre.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data = pd.read_parquet('chiller_data_pre.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data['chiller_1_evap_water_flow_rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_temp_cal = np.where(\n",
    "    chiller_data['chiller_1_evap_water_flow_rate'] == 0,\n",
    "    chiller_data['chiller_1_evap_leaving_water_temperature'],\n",
    "    24 * chiller_data['chiller_1_cooling_rate'] / chiller_data['chiller_1_evap_water_flow_rate'] + \\\n",
    "    chiller_data['chiller_1_evap_leaving_water_temperature']\n",
    ")\n",
    "\n",
    "return_temp_actual = chiller_data['chiller_1_evap_entering_water_temperature']\n",
    "\n",
    "# Add these columns to the DataFrame for plotting\n",
    "chiller_data['return_temp_cal'] = return_temp_cal\n",
    "chiller_data['return_temp_actual'] = return_temp_actual\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = px.line(chiller_data, x=chiller_data.index, y=['return_temp_cal', 'return_temp_actual'], \n",
    "              labels={'value': 'Temperature (Â°C)', 'variable': 'Temperature Type'},\n",
    "              title='Comparison of Calculated and Actual Return Temperatures')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually extract the day from the datetime string\n",
    "def extract_day(date_str):\n",
    "    return date_str.split()[0]\n",
    "\n",
    "# Create a new column with the extracted day\n",
    "chiller_data['day'] = chiller_data.index.map(extract_day)\n",
    "\n",
    "# Create the 'new_day' feature\n",
    "chiller_data['new_day'] = (chiller_data['day'] != chiller_data['day'].shift(1)).astype(int)\n",
    "\n",
    "# Drop the temporary 'day' column\n",
    "chiller_data.drop(columns=['day'], inplace=True)\n",
    "\n",
    "# Display the first few rows to check the new feature\n",
    "print(chiller_data[['new_day']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiller_data['plant_power_all_pumps'] = chiller_data['plant_power_all_chps'] + chiller_data['plant_power_all_cdps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load plant power prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "features = ['plant_cooling_rate',\n",
    "            'outdoor_weather_station_wetbulb_temperature',\n",
    "            'outdoor_weather_station_relative_humidity',\n",
    "            'chilled_water_loop_return_water_temperature',\n",
    "            'chilled_water_loop_supply_water_temperature'] + \\\n",
    "            [f'chiller_{i}_status_read' for i in range(1,6)] + \\\n",
    "            [f'chiller_{i}_setpoint_read' for i in range(1,6)] + \\\n",
    "            ['chilled_water_loop_flow_rate',\n",
    "             'condenser_water_loop_flow_rate']\n",
    "\n",
    "target = ['plant_power'] \n",
    "\n",
    "X = chiller_data[features]\n",
    "y = chiller_data[target]\n",
    "\n",
    "X_train, y_train = X[:int(0.6*len(X))], y[:int(0.6*len(y))]\n",
    "X_test, y_test = X[int(0.6*len(X)):], y[int(0.6*len(y)):]\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=400,\n",
    "                           max_depth=30,\n",
    "                           min_samples_split=10,\n",
    "                           min_samples_leaf=6, \n",
    "                           random_state=42, \n",
    "                           bootstrap=True)\n",
    "rf_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest MSE: {mse_rf}\")\n",
    "\n",
    "y_pred_rf_df = pd.DataFrame(y_pred_rf, index=y_test.index, columns=y_test.columns)\n",
    "fig = px.line(x=X_test.index, y=y_pred_rf_df[target[0]], title=f'{target[0]}')\n",
    "fig.update_traces(line_color='red', name=f'{target[0]}')\n",
    "fig.add_trace(px.line(x=X_test.index, y=y_test[target[0]]).data[0])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chiller:\n",
    "    def __init__(self, chiller_id):\n",
    "        self.chiller_id = chiller_id\n",
    "        self.power_model = joblib.load(f'chiller_{chiller_id}_power_lr_model.pkl')\n",
    "        self.chwrt_model = joblib.load(f'chiller_{chiller_id}_chwrt_lr_model.pkl')\n",
    "        self.count_on = 0\n",
    "        self.count_off = 0\n",
    "        self.mode = 0\n",
    "\n",
    "        self.feature_columns_power = [\n",
    "            f'chiller_{chiller_id}_cooling_rate',\n",
    "            f'chiller_{chiller_id}_evap_leaving_water_temperature',\n",
    "            f'chiller_{chiller_id}_evap_entering_water_temperature',\n",
    "            f'chiller_{chiller_id}_evap_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_cond_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_status_read'\n",
    "        ]\n",
    "        self.feature_columns_chwrt = [\n",
    "            f'chiller_{chiller_id}_cooling_rate',\n",
    "            f'chiller_{chiller_id}_evap_leaving_water_temperature',\n",
    "            f'chiller_{chiller_id}_evap_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_cond_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_status_read'\n",
    "        ]\n",
    "    \n",
    "    def get_power_consumption(self, state, action):\n",
    "        chiller_mode, chiller_setpoint, chwr_temp, chw_flow, cdw_flow = action\n",
    "        Crc = state\n",
    "        input_data = np.concatenate([[Crc], [chiller_setpoint, chwr_temp, chw_flow, cdw_flow, chiller_mode]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns_power)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return power.clip(min=0)\n",
    "    \n",
    "    def get_chwr_temp(self, state, action):\n",
    "        chiller_mode, chiller_setpoint, chw_flow, cdw_flow = action\n",
    "        \n",
    "        input_data = np.concatenate([[state[0]], [chiller_setpoint], [chw_flow, cdw_flow, chiller_mode]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns_chwrt)\n",
    "        chwr_temp = self.chwrt_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return chwr_temp\n",
    "    \n",
    "    def reset_status(self):\n",
    "        self.count_on = 0\n",
    "        self.count_off = 0\n",
    "    \n",
    "class Pump:\n",
    "    def __init__(self):\n",
    "        self.power_model = joblib.load('pumps_power_model.pkl')\n",
    "        \n",
    "        self.feature_columns = ['plant_cooling_rate',\n",
    "                                'chilled_water_loop_flow_rate',\n",
    "                                'condenser_water_loop_flow_rate']\n",
    "    \n",
    "    def get_power_consumption(self, state, action):\n",
    "        chw_flows, condw_flows = action\n",
    "        chw_loop_flow = sum(chw_flows)\n",
    "        cdw_loop_flow = sum(condw_flows)\n",
    "        input_data = np.concatenate([[state[0]], [chw_loop_flow, cdw_loop_flow]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return power.clip(min=0)\n",
    "\n",
    "class CoolingTower:\n",
    "    def __init__(self):\n",
    "        # Example of cooling tower power model loading\n",
    "        self.power_model = joblib.load('cts_power_model.pkl')\n",
    "        \n",
    "        self.feature_columns = [\n",
    "            'plant_cooling_rate',\n",
    "            'outdoor_weather_station_wetbulb_temperature',\n",
    "            'outdoor_weather_station_relative_humidity',\n",
    "            # 'chilled_water_loop_return_water_temperature',\n",
    "            # 'chilled_water_loop_supply_water_temperature',\n",
    "            'chilled_water_loop_flow_rate',\n",
    "            'condenser_water_loop_flow_rate'\n",
    "        ]\n",
    "    \n",
    "    def get_power_consumption(self, state, action):\n",
    "        chw_flows, condw_flows = action\n",
    "        chw_loop_flow = sum(chw_flows)\n",
    "        cdw_loop_flow = sum(condw_flows)\n",
    "        input_data = np.concatenate([state[0:3], [chw_loop_flow, cdw_loop_flow]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return power.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChillerPlantEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(data)\n",
    "        \n",
    "        # Define action and state spaces\n",
    "        self.n_chillers = 5  # Excluding standby chiller\n",
    "        self.action_space = 20  # 5 for chiller on/off, 5 for setpoints, 5 for chw_flow, 5 for condw_flow\n",
    "        self.state_space = 5\n",
    "        \n",
    "        self.chillers = [Chiller(i) for i in range(1, self.n_chillers + 1)]\n",
    "        self.pumps = Pump()\n",
    "        self.cooling_towers = CoolingTower()\n",
    "        self.consecutive_violations = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.consecutive_violations = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Decode action\n",
    "        chiller_modes, chiller_setpoints, chw_flows, condw_flows = self._decode_action(action)\n",
    "        \n",
    "        # Get current state\n",
    "        state = self._get_state()\n",
    "\n",
    "        # Update chillers status\n",
    "        operation_penalty = self._update_chillers_status(chiller_modes)\n",
    "\n",
    "        # Ccooling rate per chiller: balance cooling rate distribution\n",
    "        Crc = state[0] / sum(chiller_modes) if sum(chiller_modes) != 0 else 0\n",
    "\n",
    "        # Get chilled water return temperature for each chiller\n",
    "        chwr_temps = [chiller.get_chwr_temp(state, (mode, setpoint, chw_flow, cdw_flow)) \\\n",
    "                      for chiller, mode, setpoint, chw_flow, cdw_flow \\\n",
    "                        in zip(self.chillers, chiller_modes, chiller_setpoints, chw_flows, condw_flows)]\n",
    "        chwr_temps = [temp[0] for temp in chwr_temps]\n",
    "\n",
    "        # Calculate power consumption for each chiller, pump, and cooling tower\n",
    "        chiller_powers = [chiller.get_power_consumption(Crc, (mode, setpoint, chwr_temp, chw_flow, cdw_flow)) \\\n",
    "                          for chiller, mode, setpoint, chwr_temp, chw_flow, cdw_flow \\\n",
    "                            in zip(self.chillers, chiller_modes, chiller_setpoints, chwr_temps, chw_flows, condw_flows)]\n",
    "        pump_power = self.pumps.get_power_consumption(state, (chw_flows, condw_flows))\n",
    "        ct_power = self.cooling_towers.get_power_consumption(state, (chw_flows, condw_flows))  # No action needed for cooling tower\n",
    "        \n",
    "        total_power_consumption = sum(chiller_powers) + pump_power + ct_power\n",
    "\n",
    "        # Check constraints\n",
    "        constraints_violated, penalty = self._check_constraints(state, chiller_modes, \n",
    "                                                chiller_setpoints, operation_penalty, Crc, chwr_temps)\n",
    "        # Update consecutive violations counter\n",
    "        if constraints_violated:\n",
    "            self.consecutive_violations += 1\n",
    "        else:\n",
    "            if self.consecutive_violations > 0:\n",
    "                self.consecutive_violations -= 10\n",
    "            else:\n",
    "                self.consecutive_violations > 0\n",
    "\n",
    "        # Calculate reward\n",
    "        scaled_penalty = penalty * self.consecutive_violations\n",
    "        reward = -total_power_consumption[0] - scaled_penalty  # Large penalty for violating constraints\n",
    "        \n",
    "        # Reset On/Off count on new day\n",
    "        if state[4]:\n",
    "            for chiller in self.chillers:\n",
    "                chiller.reset_status()\n",
    "\n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "    \n",
    "    def _get_state(self):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        return np.array([\n",
    "            row['plant_cooling_rate'],\n",
    "            row['outdoor_weather_station_wetbulb_temperature'],\n",
    "            row['outdoor_weather_station_relative_humidity'],\n",
    "            row['plant_target_chw_setpoint'],\n",
    "            row['new_day']\n",
    "        ])\n",
    "    \n",
    "    def _decode_action(self, action):\n",
    "        # Decode chiller on/off states (first 5 elements of action)\n",
    "        chiller_modes = [int(action[i] > 0) for i in range(5)]\n",
    "        \n",
    "        # Decode setpoint temperatures for each chiller\n",
    "        chiller_setpoints = [43.5 + 11.5 * action[i+5] for i in range(5)]  # 32-55Â°F\n",
    "        \n",
    "        # Decode chilled/condenser water flow rate for each chiller \n",
    "        chw_flows = [1450 + 1050 * action[i+10] for i in range(5)]  # 400-2500 GPM\n",
    "        condw_flows = [1750 + 1250 * action[i+15] for i in range(5)]  # 500-3000 GPM\n",
    "        \n",
    "        return chiller_modes, chiller_setpoints, chw_flows, condw_flows\n",
    "    \n",
    "    def _check_constraints(self, state, chiller_modes, chiller_setpoints, operation_penalty, Crc, chwr_temps):\n",
    "        target_setpoint = state[3]\n",
    "        plant_cooling_rate = state[0]\n",
    "        chwrt_penalty = 0\n",
    "        for chwr_temp in chwr_temps:\n",
    "            if chwr_temp > 59:\n",
    "                chwrt_penalty += 1\n",
    "        constraints = [\n",
    "            plant_cooling_rate > sum(chiller_modes) * 800,\n",
    "            Crc > 800,\n",
    "            chwrt_penalty,\n",
    "            operation_penalty\n",
    "            # turn on/off more than once a day\n",
    "        ]\n",
    "        # penalties = {\n",
    "        #     \"cooling_capacity\": 2000 if constraints[0] else 0,\n",
    "        #     \"Crc\": 2000 if constraints[1] else 0,\n",
    "        #     \"chwr_temp\":  2000,\n",
    "        #     \"operation_penalty\": 2000 if constraints[3] else 0\n",
    "        # }\n",
    "        # total_penalty = sum(penalties.values())\n",
    "        constraints_violated = any(constraints)\n",
    "        return constraints_violated, 1\n",
    "    \n",
    "    def _update_chillers_status(self, action):\n",
    "        chiller_modes = action\n",
    "        penalty = 0\n",
    "        for i in range (len(chiller_modes)):\n",
    "            chiller = self.chillers[i]\n",
    "            if chiller.mode == 0 and chiller_modes[i]==1:\n",
    "                chiller.count_on += 1\n",
    "                if chiller.count_on > 1:\n",
    "                    penalty += chiller.count_on\n",
    "            elif chiller.mode == 1 and chiller_modes[i]==0:\n",
    "                chiller.count_off += 1\n",
    "                if chiller.count_off > 1:\n",
    "                    penalty += chiller.count_off\n",
    "            else:\n",
    "                pass\n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99  # discount rate\n",
    "        self.epsilon = 0.8  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_space, action_space).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Ensure reward is a scalar\n",
    "        if isinstance(reward, (list, np.ndarray)):\n",
    "            reward = reward[0] if len(reward) > 0 else 0.0\n",
    "        self.memory.append((np.array(state), np.array(action), float(reward), np.array(next_state), done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.uniform(-1, 1, 20).tolist()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return act_values.cpu().data.numpy()[0].tolist()  # Returns 20 values (action space)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        # Convert the batch to numpy arrays\n",
    "        states = np.array([transition[0] for transition in minibatch])\n",
    "        actions = np.array([transition[1] for transition in minibatch])\n",
    "        rewards = np.array([transition[2] for transition in minibatch], dtype=np.float32)\n",
    "        next_states = np.array([transition[3] for transition in minibatch])\n",
    "        dones = np.array([transition[4] for transition in minibatch], dtype=np.float32)\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        current_q_values = self.model(states)\n",
    "\n",
    "        # Compute Q values for next states\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "\n",
    "        # Compute target Q values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(current_q_values, actions * target_q_values.unsqueeze(1) + (1 - actions) * current_q_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_agent(episodes, batch_size):\n",
    "\n",
    "    data = pd.read_parquet('chiller_data_pre.parquet')\n",
    "    data = data[:int(0.6*len(data))]\n",
    "    \n",
    "    env = ChillerPlantEnvironment(data)\n",
    "    agent = DQNAgent(env.state_space, env.action_space)\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for time in range(env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            print(f\"Timestep: {time}, Action: {action}, Reward: {reward}\")\n",
    "            if done:\n",
    "                print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "                break\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        \n",
    "        # Save the model periodically\n",
    "        if e % 100 == 0:\n",
    "            torch.save(agent.model.state_dict(), f'chiller_dqn_model_{e}.pth')\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate_agent(agent, episodes):\n",
    "\n",
    "    data = pd.read_parquet('chiller_data_pre.parquet')\n",
    "    data = data[int(0.6*len(data)):]\n",
    "    \n",
    "    env = ChillerPlantEnvironment(data)\n",
    "    agent.epsilon = 0  # No exploration during evaluation\n",
    "    \n",
    "    total_rewards = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        \n",
    "        for time in range(env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            actions.append(action)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Evaluation Episode: {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    print(f\"Average Reward over {episodes} episodes: {np.mean(total_rewards)}\")\n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the agent\n",
    "trained_agent = train_agent(episodes=1000, batch_size=32)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "optimal_actions = evaluate_agent(trained_agent, episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chiller:\n",
    "    def __init__(self, chiller_id):\n",
    "        self.chiller_id = chiller_id\n",
    "        self.power_model = joblib.load(f'chiller_{chiller_id}_power_lr_model.pkl')\n",
    "        self.chwrt_model = joblib.load(f'chiller_{chiller_id}_chwrt_lr_model.pkl')\n",
    "        self.count_on = 0\n",
    "        self.count_off = 0\n",
    "        self.mode = 0\n",
    "\n",
    "        self.feature_columns_power = [\n",
    "            f'chiller_{chiller_id}_cooling_rate',\n",
    "            f'chiller_{chiller_id}_evap_leaving_water_temperature',\n",
    "            f'chiller_{chiller_id}_evap_entering_water_temperature',\n",
    "            f'chiller_{chiller_id}_evap_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_cond_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_status_read'\n",
    "        ]\n",
    "        self.feature_columns_chwrt = [\n",
    "            f'chiller_{chiller_id}_cooling_rate',\n",
    "            f'chiller_{chiller_id}_evap_leaving_water_temperature',\n",
    "            f'chiller_{chiller_id}_evap_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_cond_water_flow_rate',\n",
    "            f'chiller_{chiller_id}_status_read'\n",
    "        ]\n",
    "    \n",
    "    def get_power_consumption(self, state, action):\n",
    "        chiller_mode, chiller_setpoint, chwr_temp, chw_flow, cdw_flow = action\n",
    "        Crc = state\n",
    "        input_data = np.concatenate([[Crc], [chiller_setpoint, chwr_temp, chw_flow, cdw_flow, chiller_mode]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns_power)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return power.clip(min=0)\n",
    "    \n",
    "    def get_chwr_temp(self, state, action):\n",
    "        chiller_mode, chiller_setpoint, chw_flow, cdw_flow = action\n",
    "        \n",
    "        input_data = np.concatenate([[state[0]], [chiller_setpoint], [chw_flow, cdw_flow, chiller_mode]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns_chwrt)\n",
    "        chwr_temp = self.chwrt_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return chwr_temp\n",
    "    \n",
    "    def reset_status(self):\n",
    "        self.count_on = 0\n",
    "        self.count_off = 0\n",
    "    \n",
    "class Pump:\n",
    "    def __init__(self):\n",
    "        self.power_model = joblib.load('pumps_power_model.pkl')\n",
    "        \n",
    "        self.feature_columns = ['plant_cooling_rate',\n",
    "                                'chilled_water_loop_flow_rate',\n",
    "                                'condenser_water_loop_flow_rate']\n",
    "    \n",
    "    def get_power_consumption(self, state, action):\n",
    "        chw_flows, condw_flows = action\n",
    "        chw_loop_flow = sum(chw_flows)\n",
    "        cdw_loop_flow = sum(condw_flows)\n",
    "        input_data = np.concatenate([[state[0]], [chw_loop_flow, cdw_loop_flow]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return power.clip(min=0)\n",
    "\n",
    "class CoolingTower:\n",
    "    def __init__(self):\n",
    "        # Example of cooling tower power model loading\n",
    "        self.power_model = joblib.load('cts_power_model.pkl')\n",
    "        \n",
    "        self.feature_columns = [\n",
    "            'plant_cooling_rate',\n",
    "            'outdoor_weather_station_wetbulb_temperature',\n",
    "            'outdoor_weather_station_relative_humidity',\n",
    "            # 'chilled_water_loop_return_water_temperature',\n",
    "            # 'chilled_water_loop_supply_water_temperature',\n",
    "            'chilled_water_loop_flow_rate',\n",
    "            'condenser_water_loop_flow_rate'\n",
    "        ]\n",
    "    \n",
    "    def get_power_consumption(self, state, action):\n",
    "        chw_flows, condw_flows = action\n",
    "        chw_loop_flow = sum(chw_flows)\n",
    "        cdw_loop_flow = sum(condw_flows)\n",
    "        input_data = np.concatenate([state[0:3], [chw_loop_flow, cdw_loop_flow]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        return power.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class ChillerPlantEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(data)\n",
    "        self.n_chillers = 5\n",
    "        self.state_space = 5\n",
    "        self.action_space = 20  # Keep the same number of actions\n",
    "        self.action_bins = {\n",
    "            'chiller_modes': [0, 1],  # Binary\n",
    "            'chiller_setpoints': np.linspace(43.5, 55, 5),  # 5 discrete values\n",
    "            'chw_flows': np.linspace(400, 2500, 5),  # 5 discrete values\n",
    "            'condw_flows': np.linspace(500, 3000, 5)  # 5 discrete values\n",
    "        }\n",
    "        \n",
    "        # Add state normalization\n",
    "        self.state_scaler = MinMaxScaler()\n",
    "        self.state_scaler.fit(self.data[['plant_cooling_rate', 'outdoor_weather_station_wetbulb_temperature', \n",
    "                                         'outdoor_weather_station_relative_humidity', 'plant_target_chw_setpoint', 'new_day']])\n",
    "        \n",
    "        self.chillers = [Chiller(i) for i in range(1, self.n_chillers + 1)]\n",
    "        self.pumps = Pump()\n",
    "        self.cooling_towers = CoolingTower()\n",
    "        self.consecutive_violations = 0\n",
    "        self.max_consecutive_violations = 50\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.consecutive_violations = 0\n",
    "        return self._normalize_state(self._get_state())\n",
    "\n",
    "    def step(self, action):\n",
    "        # Decode action\n",
    "        chiller_modes, chiller_setpoints, chw_flows, condw_flows = self._decode_action(action)\n",
    "\n",
    "        # Get current state\n",
    "        state = self._get_state()\n",
    "\n",
    "        # Update chillers status\n",
    "        operation_penalty = self._update_chillers_status(chiller_modes)\n",
    "\n",
    "        # Ccooling rate per chiller: balance cooling rate distribution\n",
    "        Crc = state[0] / sum(chiller_modes) if sum(chiller_modes) != 0 else 0\n",
    "\n",
    "        # Get chilled water return temperature for each chiller\n",
    "        chwr_temps = [chiller.get_chwr_temp(state, (mode, setpoint, chw_flow, cdw_flow)) \\\n",
    "                      for chiller, mode, setpoint, chw_flow, cdw_flow \\\n",
    "                        in zip(self.chillers, chiller_modes, chiller_setpoints, chw_flows, condw_flows)]\n",
    "        chwr_temps = [temp[0] for temp in chwr_temps]\n",
    "\n",
    "        # Calculate power consumption for each chiller, pump, and cooling tower\n",
    "        chiller_powers = [chiller.get_power_consumption(Crc, (mode, setpoint, chwr_temp, chw_flow, cdw_flow)) \\\n",
    "                          for chiller, mode, setpoint, chwr_temp, chw_flow, cdw_flow \\\n",
    "                            in zip(self.chillers, chiller_modes, chiller_setpoints, chwr_temps, chw_flows, condw_flows)]\n",
    "        pump_power = self.pumps.get_power_consumption(state, (chw_flows, condw_flows))\n",
    "        ct_power = self.cooling_towers.get_power_consumption(state, (chw_flows, condw_flows))  # No action needed for cooling tower\n",
    "        \n",
    "        total_power_consumption = sum(chiller_powers) + pump_power + ct_power\n",
    "\n",
    "        # Check constraints\n",
    "        constraints_violated, penalty = self._check_constraints(state, chiller_modes, \n",
    "                                                chiller_setpoints, operation_penalty, Crc, chwr_temps)\n",
    "        # Update consecutive violations counter\n",
    "        if constraints_violated:\n",
    "            self.consecutive_violations += 1\n",
    "        else:\n",
    "            if self.consecutive_violations > 0:\n",
    "                self.consecutive_violations -= 10\n",
    "            else:\n",
    "                self.consecutive_violations > 0\n",
    "\n",
    "        # Calculate reward\n",
    "        scaled_penalty = penalty * self.consecutive_violations\n",
    "        reward = -total_power_consumption[0] - scaled_penalty  # Large penalty for violating constraints\n",
    "\n",
    "        # Scale reward\n",
    "        reward = reward / 1000  # Assuming typical power consumption is in the thousands\n",
    "\n",
    "         # Reset On/Off count on new day\n",
    "        if state[4]:\n",
    "            for chiller in self.chillers:\n",
    "                chiller.reset_status()\n",
    "\n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        return self._normalize_state(self._get_state()), reward, done\n",
    "\n",
    "    def _normalize_state(self, state):\n",
    "        normalized_state = self.state_scaler.transform(state.reshape(1, -1)).flatten()\n",
    "        return normalized_state  # Append the 'new_day' boolean\n",
    "\n",
    "    def _decode_action(self, action):\n",
    "        chiller_modes = [self.action_bins['chiller_modes'][int(action[i])] for i in range(5)]\n",
    "        chiller_setpoints = [self.action_bins['chiller_setpoints'][int(action[i+5])] for i in range(5)]\n",
    "        chw_flows = [self.action_bins['chw_flows'][int(action[i+10])] for i in range(5)]\n",
    "        condw_flows = [self.action_bins['condw_flows'][int(action[i+15])] for i in range(5)]\n",
    "        \n",
    "        return chiller_modes, chiller_setpoints, chw_flows, condw_flows\n",
    "\n",
    "    def _get_state(self):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        return np.array([\n",
    "            row['plant_cooling_rate'],\n",
    "            row['outdoor_weather_station_wetbulb_temperature'],\n",
    "            row['outdoor_weather_station_relative_humidity'],\n",
    "            row['plant_target_chw_setpoint'],\n",
    "            row['new_day']\n",
    "        ])\n",
    "    \n",
    "    def _check_constraints(self, state, chiller_modes, chiller_setpoints, operation_penalty, Crc, chwr_temps):\n",
    "        target_setpoint = state[3]\n",
    "        plant_cooling_rate = state[0]\n",
    "        chwrt_penalty = 0\n",
    "        for chwr_temp in chwr_temps:\n",
    "            if chwr_temp > 59:\n",
    "                chwrt_penalty += 1\n",
    "        constraints = [\n",
    "            plant_cooling_rate > sum(chiller_modes) * 800,\n",
    "            Crc > 800,\n",
    "            chwrt_penalty,\n",
    "            operation_penalty]\n",
    "        constraints_violated = any(constraints)\n",
    "        return constraints_violated, 1\n",
    "    \n",
    "    def _update_chillers_status(self, action):\n",
    "        chiller_modes = action\n",
    "        penalty = 0\n",
    "        for i in range (len(chiller_modes)):\n",
    "            chiller = self.chillers[i]\n",
    "            if chiller.mode == 0 and chiller_modes[i]==1:\n",
    "                chiller.count_on += 1\n",
    "                if chiller.count_on > 1:\n",
    "                    penalty += chiller.count_on\n",
    "            elif chiller.mode == 1 and chiller_modes[i]==0:\n",
    "                chiller.count_off += 1\n",
    "                if chiller.count_off > 1:\n",
    "                    penalty += chiller.count_off\n",
    "            else:\n",
    "                pass\n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99  # discount rate\n",
    "        self.epsilon = 0.99  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.tau = 0.001  # for soft update of target parameters\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.model = DQN(state_space, action_space).to(self.device)\n",
    "        self.target_model = DQN(state_space, action_space).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Epsilon-greedy exploration\n",
    "            action = np.zeros(self.action_space)\n",
    "            # First 5 actions are discrete (0 or 1)\n",
    "            action[:5] = np.random.randint(0, 2, 5)\n",
    "            # Remaining actions are continuous between -1 and 1\n",
    "            action[5:] = np.random.uniform(-1, 1, self.action_space - 5)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state)\n",
    "            action = q_values.cpu().data.numpy()[0]\n",
    "            # Discretize the first 5 actions\n",
    "            action[:5] = (action[:5] > 0).astype(int)\n",
    "            # Clip the continuous actions to [-1, 1]\n",
    "            action[5:] = np.clip(action[5:], -1, 1)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Get Q values for current states using online network\n",
    "        current_q_values = self.model(states)\n",
    "\n",
    "        # Separate handling for discrete and continuous actions\n",
    "        discrete_q_values = current_q_values[:, :5]\n",
    "        continuous_q_values = current_q_values[:, 5:]\n",
    "\n",
    "        discrete_actions = actions[:, :5]\n",
    "        continuous_actions = actions[:, 5:]\n",
    "\n",
    "        # Compute Q values for taken actions\n",
    "        discrete_action_q_values = torch.sum(discrete_q_values * discrete_actions, dim=1)\n",
    "        continuous_action_q_values = torch.sum(continuous_q_values * continuous_actions, dim=1)\n",
    "        taken_action_q_values = discrete_action_q_values + continuous_action_q_values\n",
    "\n",
    "        # Double DQN: Use online network to select actions, target network to evaluate\n",
    "        with torch.no_grad():\n",
    "            # Select actions using online network\n",
    "            online_next_q_values = self.model(next_states)\n",
    "            best_discrete_actions = torch.argmax(online_next_q_values[:, :5], dim=1)\n",
    "            best_continuous_actions = online_next_q_values[:, 5:]\n",
    "            \n",
    "            # Evaluate Q-values using target network\n",
    "            target_next_q_values = self.target_model(next_states)\n",
    "            best_discrete_q_values = target_next_q_values[:, :5].gather(1, best_discrete_actions.unsqueeze(1)).squeeze(1)\n",
    "            best_continuous_q_values = torch.sum(target_next_q_values[:, 5:] * best_continuous_actions, dim=1)\n",
    "            best_q_values = best_discrete_q_values + best_continuous_q_values\n",
    "\n",
    "        # Compute target Q values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * best_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(taken_action_q_values, target_q_values.detach())\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update target network\n",
    "        self.soft_update(self.model, self.target_model)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_agent(episodes, batch_size):\n",
    "    data = pd.read_parquet('chiller_data_pre.parquet')\n",
    "    data = data[:int(0.6*len(data))]\n",
    "    \n",
    "    env = ChillerPlantEnvironment(data)\n",
    "    agent = DoubleDQNAgent(env.state_space, env.action_space)\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for time in range(env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            print(f\"Episode: {e+1}, Timestep: {time}, Action: {action}, Reward: {reward}\")\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.4f}\")\n",
    "        \n",
    "        # Save the model periodically\n",
    "        if (e + 1) % 100 == 0:\n",
    "            agent.save(f'double_dqn_model_{e+1}.pth')\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate_agent(agent, episodes):\n",
    "    data = pd.read_parquet('chiller_data_pre.parquet')\n",
    "    data = data[int(0.6*len(data)):]\n",
    "    \n",
    "    env = ChillerPlantEnvironment(data)\n",
    "    agent.epsilon = 0  # No exploration during evaluation\n",
    "    \n",
    "    total_rewards = []\n",
    "    all_actions = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_actions = []\n",
    "        \n",
    "        for time in range(env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_actions.append(action)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        all_actions.append(episode_actions)\n",
    "        print(f\"Evaluation Episode: {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    print(f\"Average Reward over {episodes} episodes: {np.mean(total_rewards)}\")\n",
    "    return all_actions\n",
    "\n",
    "# Usage example\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# Train the agent\n",
    "trained_agent = train_agent(episodes, batch_size)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "evaluation_actions = evaluate_agent(trained_agent, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class ChillerPlantEnvironment:\n",
    "    def __init__(self, data):\n",
    "        # self.data = data\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(data)\n",
    "        \n",
    "        self.n_chillers = 5\n",
    "        self.action_space = 12\n",
    "        self.state_space = 5\n",
    "        \n",
    "        self.power_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "        self.feature_columns = ['plant_cooling_rate',\n",
    "            'outdoor_weather_station_wetbulb_temperature',\n",
    "            'outdoor_weather_station_relative_humidity',\n",
    "            'chilled_water_loop_return_water_temperature',\n",
    "            'chilled_water_loop_supply_water_temperature'] + \\\n",
    "            [f'chiller_{i}_status_read' for i in range(1, 6)] + \\\n",
    "            [f'chiller_{i}_setpoint_read' for i in range(1, 6)] + \\\n",
    "            ['chilled_water_loop_flow_rate',\n",
    "             'condenser_water_loop_flow_rate']\n",
    "        self.data = data[self.feature_columns]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        chiller_modes, chiller_setpoints, chw_flow, condw_flow = self._decode_action(action)\n",
    "        \n",
    "        state = self._get_state()\n",
    "\n",
    "        input_data = np.concatenate([state, chiller_modes, chiller_setpoints, [chw_flow], [condw_flow]]).reshape(1, -1)\n",
    "        input_data_df = pd.DataFrame(input_data, columns=self.feature_columns)\n",
    "        power = self.power_model.predict(input_data_df)[0]\n",
    "        \n",
    "        cooling_capacity = self._calculate_cooling_capacity(chiller_modes)\n",
    "        constraints_violated = self._check_constraints(chiller_modes, chw_flow, condw_flow)\n",
    "        \n",
    "        reward = -power if not constraints_violated else -1000\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "    \n",
    "    def _get_state(self):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        return np.array([\n",
    "            row['plant_cooling_rate'],\n",
    "            row['outdoor_weather_station_wetbulb_temperature'],\n",
    "            row['outdoor_weather_station_relative_humidity'],\n",
    "            row['chilled_water_loop_return_water_temperature'],\n",
    "            row['chilled_water_loop_supply_water_temperature'],\n",
    "        ])\n",
    "    \n",
    "    def _decode_action(self, action):\n",
    "        chiller_modes = [int(action[i] > 0) for i in range(5)]\n",
    "        chiller_setpoints = [42 + 9 * action[i+5] for i in range(5)]\n",
    "        chw_flow = 400 + 2100 * action[10]\n",
    "        condw_flow = 500 + 2500 * action[11]\n",
    "        \n",
    "        return chiller_modes, chiller_setpoints, chw_flow, condw_flow\n",
    "    \n",
    "    def _calculate_cooling_capacity(self, chiller_modes):\n",
    "        return sum(chiller_modes) * 800\n",
    "    \n",
    "    def _check_constraints(self, chiller_modes, chw_flow, condw_flow):\n",
    "        cooling_load = self.data.iloc[self.current_step]['plant_cooling_rate']\n",
    "        cooling_capacity = self._calculate_cooling_capacity(chiller_modes)\n",
    "        chwr_temp = self.data.iloc[self.current_step]['chilled_water_loop_return_water_temperature']\n",
    "        constraints = [\n",
    "            cooling_capacity < cooling_load,\n",
    "            chw_flow < 400 or chw_flow > 2500,\n",
    "            condw_flow < 500 or condw_flow > 3000,\n",
    "            chwr_temp > 59,\n",
    "        ]\n",
    "        return any(constraints)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_space, action_space).to(self.device)\n",
    "        self.target_model = DQN(state_space, action_space).to(self.device)\n",
    "        self.update_target_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.uniform(-1, 1, 5).tolist() + np.random.uniform(0, 1, 7).tolist()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return act_values.cpu().data.numpy()[0]\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            action = torch.FloatTensor(action).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            target = reward\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    target = reward + self.gamma * torch.max(self.target_model(next_state))\n",
    "            \n",
    "            target_f = self.model(state)\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(target_f, self.model(state))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def train_agent(episodes, batch_size):\n",
    "    data = pd.read_parquet('chiller_data_pre.parquet')\n",
    "    data = data[:int(0.6*len(data))]\n",
    "    \n",
    "    env = ChillerPlantEnvironment(data)\n",
    "    agent = DQNAgent(env.state_space, env.action_space)\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for time in range(env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "                break\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        \n",
    "        agent.update_target_model()\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "            torch.save(agent.model.state_dict(), f'chiller_dqn_model_{e}.pth')\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate_agent(agent, episodes):\n",
    "    data = pd.read_parquet('chiller_data_pre.parquet')\n",
    "    data = data[int(0.6*len(data)):]\n",
    "    \n",
    "    env = ChillerPlantEnvironment(data)\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    total_rewards = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        \n",
    "        for time in range(env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            actions.append(action)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Evaluation Episode: {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    print(f\"Average Reward over {episodes} episodes: {np.mean(total_rewards)}\")\n",
    "    return actions\n",
    "\n",
    "# Train the agent\n",
    "trained_agent = train_agent(episodes=1000, batch_size=32)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "optimal_actions = evaluate_agent(trained_agent, episodes=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altotech-ml-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
